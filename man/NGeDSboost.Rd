% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NGeDSboost.R
\name{NGeDSboost}
\alias{NGeDSboost}
\title{Component-wise gradient boosting with NGeDS base-learners}
\usage{
NGeDSboost(
  formula,
  data,
  weights = NULL,
  normalize_data = FALSE,
  family = mboost::Gaussian(),
  initial_learner = TRUE,
  int.knots_init = 2L,
  min_iterations = 0L,
  max_iterations = 100L,
  shrinkage = 1,
  phi_boost_exit = 0.995,
  q_boost = 2L,
  beta = 0.5,
  phi = 0.99,
  internal_knots = 500L,
  q = 2L,
  higher_order = TRUE
)
}
\arguments{
\item{formula}{a description of the structure of the model to be fitted, including the
dependent and independent variables. See \code{\link[=formula.GeDS]{formula}} for details.}

\item{data}{a data frame containing the variables of the model.}

\item{weights}{an optional vector of `prior weights' to be put on the observations in the fitting
process in case the user requires weighted GeDS fitting. It should be \code{NULL} or a numeric vector of the same length as the response
variable in the argument \code{\link[=formula.GeDS]{formula}}.}

\item{normalize_data}{a logical that defines whether the data should be normalized (standardized)
before fitting the model or not.}

\item{family}{specifies the loss function to be used in the boosting model. By default, it's set to \code{"gaussian"}.
Users can specify any \link[mboost]{Family} object from the \code{mboost} package.}

\item{initial_learner}{a logical value. If set to \code{TRUE}, the initial learner will be a GeD spline. 
When set to FALSE, it defaults to the empirical risk minimizer of the specified family. Note: If 
\code{initial_learner} is set to \code{TRUE}, \code{family} must be \code{"gaussian"}.}

\item{int.knots_init}{optional parameter allowing the user to set a maximum number
of internal knots to be added by the initial GeDS learner in case \code{initial_learner = TRUE}. Default is equal to \code{2L}.}

\item{min_iterations}{optional parameter allowing the user to set a minimum number
of boosting iterations.}

\item{max_iterations}{optional parameter allowing the user to set a maximum number
of boosting iterations.}

\item{shrinkage}{numeric parameter in the interval \eqn{[0,1]} defining the step size or shrinkage parameter.}

\item{phi_boost_exit}{numeric parameter in the interval \eqn{[0,1]} specifying the threshold for
the boosting iterations stopping rule.}

\item{q_boost}{numeric parameter which allows to fine-tune the boosting iterations stopping rule,
by default equal to \code{2L}.}

\item{beta}{numeric parameter in the interval \eqn{[0,1]}
tuning the knot placement in stage A of GeDS. See details in \code{\link{NGeDS}}.}

\item{phi}{numeric parameter in the interval \eqn{[0,1]} specifying the threshold for
the stopping rule  (model selector) in stage A of GeDS. See also \code{stoptype} and details in
\code{\link{NGeDS}}.}

\item{internal_knots}{The maximum number of internal knots that can be added by the GeDS 
base-learners in each boosting iteration, effectively setting the value of \code{max.intknots} 
in \code{\link{NGeDS}}. Default is \code{500L}.}

\item{q}{numeric parameter which allows to fine-tune the stopping rule of stage A of GeDS, by default equal to \code{2L}.
See details in \code{\link{NGeDS}}.}

\item{higher_order}{a logical that defines whether to compute the higher order fits (quadratic and cubic), default is \code{TRUE}.

#' @return \code{\link{GeDSboost-Class}} object, i.e. a list of items that summarizes the main
details of the fitted FGB-GeDS regression. See \code{\link{GeDSboost-Class}} for details.
Some S3 methods are available in order to make these objects tractable, such as \code{\link[=predict.GeDSboost]{predict}}
as well as S4 methods such as \code{\link{visualize_boosting}}.}
}
\description{
\code{NGeDSboost} performs component-wise gradient boosting  
using GeD splines, for a response having a Normal distribution, as base-learners.
}
\details{
The  \code{NGeDSboost} function performs functional gradient boosting 
for some pre-defined loss function and GeD splines as base learners. 
At each boosting iteration, the negative gradient vector is fitted using the base procedure
encapsulated within the \code{\link{NGeDS}} function. The latter constructs a
Geometrically Designed variable knots spline regression model for a response having a
Normal distribution.

On the one hand, \code{NGeDSboost} includes all the parameters of \code{\link{NGeDS}},
which in this case tune the base-learner fit at each boosting iteration. On the other hand,
\code{NGeDSboost} includes some additional parameters proper to the FGB procedure. We describe
the main ones as follows. 

First, \code{family} allows to specify the loss function and corresponding risk
function to be optimized by the boosting algorithm. If \code{initial_learner = FALSE},
the initial learner used will be the empirical risk minimizer corresponding to the family
chosen. If \code{initial_learner = TRUE} then the initial learner will be an \code{\link{NGeDS}}
fit with maximum number of internal knots equal to \code{int.knots_init}.


\code{shrinkage} tunes the step length/shrinkage parameter which
helps to control the learning rate of the model. In other words, when a new base learner is 
added to the ensemble, its contribution to the final prediction is multiplied by the
shrinkage parameter. This results in a slower, more gradual learning process. 
The number of boosting iterations is controlled by a \emph{Ratio of Deviances} stopping rule 
similar to the one presented for \code{\link{GGeDS}}.
In the same way \code{phi} and \code{q} tune the stopping rule of \code{\link{NGeDS}},
\code{phi_boost_exit} and \code{q_boost} tune the stopping rule of \code{NGeDSboost}. The user
can also manually control the number of boosting iterations through \code{min_iterations} and
\code{max_iterations}.
}
\examples{

################################# Example 1 #################################
# Generate a data sample for the response variable
# Y and the single covariate X
set.seed(123)
N <- 500
f_1 <- function(x) (10*x/(1+100*x^2))*4+4
X <- sort(runif(N, min = -2, max = 2))
# Specify a model for the mean of Y to include only a component
# non-linear in X, defined by the function f_1
means <- f_1(X)
# Add (Normal) noise to the mean of Y
Y <- rnorm(N, means, sd = 0.2)
data = data.frame(X, Y)

# Fit a Normal FGB-GeDS regression using NGeDSboost

Gmodboost <- NGeDSboost(Y ~ f(X), data = data)
MSE_Gmodboost_linear <- mean((sapply(X, f_1) - Gmodboost$predictions$pred_linear)^2)
MSE_Gmodboost_quadratic <- mean((sapply(X, f_1) - Gmodboost$predictions$pred_quadratic)^2)
MSE_Gmodboost_cubic <- mean((sapply(X, f_1) - Gmodboost$predictions$pred_cubic)^2)

cat("\n", "MEAN SQUARED ERROR", "\n",
"Linear NGeDSboost:", MSE_Gmodboost_linear, "\n",
"Quadratic NGeDSboost:", MSE_Gmodboost_quadratic, "\n",
"Cubic NGeDSboost:", MSE_Gmodboost_cubic, "\n")

# Compute predictions on new randomly generated data
X <- sort(runif(100, min = -2, max = 2))

pred_linear <- predict(Gmodboost, newdata = data.frame(X), n = 2L)
pred_quadratic <- predict(Gmodboost, newdata = data.frame(X), n = 3L)
pred_cubic <- predict(Gmodboost, newdata = data.frame(X), n = 4L)

MSE_Gmodboost_linear <- mean((sapply(X, f_1) - pred_linear)^2)
MSE_Gmodboost_quadratic <- mean((sapply(X, f_1) - pred_quadratic)^2)
MSE_Gmodboost_cubic <- mean((sapply(X, f_1) - pred_cubic)^2)
cat("\n", "MEAN SQUARED ERROR", "\n",
"Linear NGeDSboost:", MSE_Gmodboost_linear, "\n",
"Quadratic NGeDSboost:", MSE_Gmodboost_quadratic, "\n",
"Cubic NGeDSboost:", MSE_Gmodboost_cubic, "\n")

############################ Example 2 - Bodyfat ############################
library(TH.data)
data("bodyfat", package = "TH.data")

Gmodboost <- NGeDSboost(formula = DEXfat ~ age + f(hipcirc, waistcirc) + f(kneebreadth),
data = bodyfat)

MSE_Gmodboost_linear <- mean((bodyfat$DEXfat - Gmodboost$predictions$pred_linear)^2)
MSE_Gmodboost_quadratic <- mean((bodyfat$DEXfat - Gmodboost$predictions$pred_quadratic)^2)
MSE_Gmodboost_cubic <- mean((bodyfat$DEXfat - Gmodboost$predictions$pred_cubic)^2)
# Comparison
cat("\n", "MSE", "\n",
    "Linear NGeDSboost:", MSE_Gmodboost_linear, "\n",
    "Quadratic NGeDSboost:", MSE_Gmodboost_quadratic, "\n",
    "Cubic NGeDSboost:", MSE_Gmodboost_cubic, "\n")
}
\references{
Dimitrova, D. S. , Kaishev, V. K., Lattuada, A. view all authors (2023).
Geometrically designed variable knot splines in generalized (non-)linear models.
\emph{Applied Mathematics and Computation}, \strong{436}. \cr
DOI: \href{https://doi.org/10.1016/j.amc.2022.127493}{doi.org/10.1016/j.amc.2022.127493}

Friedman, J.H. (2001).
Greedy function approximation: A gradient boosting machine.
\emph{The Annals of Statistics}, \strong{29 (5)}, 1189--1232. \cr
DOI: \href{https://doi.org/10.1214/aos/1013203451}{doi.org/10.1214/aos/1013203451}

Kaishev, V.K., Dimitrova, D.S., Haberman, S. and Verrall, R.J. (2016).
Geometrically designed, variable knot regression splines.
\emph{Computational Statistics}, \strong{31}, 1079--1105. \cr
DOI: \href{https://doi.org/10.1007/s00180-015-0621-7}{doi.org/10.1007/s00180-015-0621-7}
}
\seealso{
\code{\link{NGeDS}}; \code{\link{GGeDS}}; \code{\link{GeDSboost-Class}}; S3 methods such as
\code{\link{predict.GeDSboost}}; \code{\link{coef.GeDSboost}}
}
